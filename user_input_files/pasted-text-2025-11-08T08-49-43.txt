# Plan Maestro — Plataforma Multi‑Agente HAAS + MCP (multi‑tenant)

**Versión:** 1.1 (ejecutado desde Prompt PMO)
**Autor:** Equipo PMO de Plataforma (arch., SRE, datos, producto, UX/UI, SecOps, QA, tech writer) para AHFB
**Fecha:** 08‑Nov‑2025 (America/Lima)

---

## 0) Prompt base (para regeneraciones automatizadas)

> Quiero que actúes como un equipo PMO de plataforma (arquitecto de software, SRE, líder de datos, líder de producto, diseñador UX/UI, SecOps, QA y technical writer). Tu tarea es producir un Plan Maestro exhaustivo para una plataforma multi‑agente jerárquica (HAAS) + servidor MCP multi‑tenant que será consumida por múltiples apps (NWC, MedLuxe, Brandistry, Silhouette, etc.). Debe incluir: alcance, requisitos, arquitectura lógica y física, decisiones de diseño (con pros/cons), plan por fases y cronograma, historias de usuario, APIs (REST/gRPC), contratos, DDL SQL, esquemas JSON/YAML, flujos ES/CQRS, grafos de dependencias, RAG por tenant, MCP multi‑tenant, seguridad (RLS/OPA/mTLS), sandboxing, colas, observabilidad, procedimiento de despliegue (VPS 4 vCPU/8 GB), plan de pruebas (unitarias, integración, e2e), plan de hardening, manual de operación, guías de UI/UX y design system, SDKs cliente (C#/.NET, JS/TS, Python), guía de integración para apps existentes, checklists, riesgos/mitigaciones, y hoja de ruta de escalado. Salida en Markdown con secciones numeradas, tablas y diagramas Mermaid. Incluye plantillas para profile.yaml, brief.md, team_report.md, eventos ES, ejemplos de CTE recursivo para dependencias, y docker-compose minimizado. El resultado debe ser ejecutable y orientado a multi‑tenant: todo request obligatoriamente lleva {tenant_id, project_id} y todo dato aplica RLS.

---

## 1) Resumen Ejecutivo

* Plataforma centralizada **HAAS + MCP** compartida por todas tus apps.
* **Multi‑tenant** real: aislamiento por `{tenant_id, project_id}` en **API, DB, RAG, artefactos y eventos**.
* **Memoria auditable**: Event Sourcing (ES) en Postgres; CQRS con **read models** y **consultas recursivas**; ruta de ascenso a **Neo4j**.
* **Orquestación jerárquica**: Orquestador → Ingeniero de Prompts (CoT sombra + APE) → Planificador (DAG con contratos I/O) → Líderes → Especialistas.
* **MCP** multi‑tenant con **policy engine** (scopes, quotas, rate‑limits, mutex) y **sandbox** de herramientas.
* **RAG**: `pgvector` por tenant/proyecto; embeddings versionadas; re‑rank opcional.
* **UI/UX**: consola unificada (estado de planes, artefactos, alertas) con design system y accesibilidad.
* **Despliegue**: optimizado para 8 GB RAM; *upgrade path* a microservicios y grafos dedicados.

---

## 2) Alcance y Requisitos

### 2.1 Requisitos funcionales

1. Orquestar intenciones → **briefs** → **planes DAG**.
2. Ejecutar tareas con **artefactos `.md`** obligatorios (estigmergia + hash encadenado).
3. **Memoria ES/CQRS** + **modelo de dependencias** consultable (CTE recursivo o Cypher).
4. **MCP** multi‑tenant: catálogo de tools por perfil; **rate‑limit/quotas/mutex**.
5. **Notificaciones dirigidas** por dependencias (sin broadcast masivo).
6. **APIs públicas** REST/gRPC (intents, planes, ejecución, artefactos, tools).
7. **RAG** segregado por tenant/proyecto.
8. Consola **UI** de estado, métricas, auditoría y locks.

### 2.2 Requisitos no funcionales

* Seguridad: **RLS**, JWT con claims, mTLS opcional, cifrado en tránsito y reposo.
* Observabilidad: logs/métricas/trazas etiquetadas `{tenant_id, project_id, plan_id, task_id}`.
* Resiliencia: reintentos, circuit breakers, backoff, sandboxing.
* Costo/Footprint: Postgres/Redis priorizados; servicios compactos; backups.

---

## 3) Arquitectura (lógica)

```mermaid
flowchart LR
  subgraph Clients/Apps
    APP1[NWC]
    APP2[MedLuxe]
    APP3[Brandistry]
    APP4[Silhouette]
  end

  subgraph Gateway
    GW[API Gateway REST/gRPC + Auth]
  end

  subgraph HAAS Core
    ORQ[Orquestador (Intents/Policy)]
    ENG[Ingeniería de Prompts]
    PLAN[Planificador (DAG Scheduler)]
    NOTI[Notificaciones]
  end

  subgraph Memory
    ES[(Event Store: Postgres)]
    RM[(Read Models: Postgres)]
    VDB[(pgvector por tenant/proyecto)]
  end

  subgraph Tools via MCP
    MCP[MCP Server Multi‑tenant]
    SAN[Sandbox Runner]
    TP[Tool Proxies]
  end

  APP1-->GW-->ORQ-->ENG-->PLAN
  PLAN-->MCP
  MCP-->SAN
  PLAN-- events -->ES
  ORQ-- queries -->RM
  PLAN-- RAG -->VDB
  NOTI-->Clients
```

### 3.1 Decisiones clave

| Tema        | Opción elegida                  | Razón                   | Alternativas                |
| ----------- | ------------------------------- | ----------------------- | --------------------------- |
| Event Store | Postgres tabla append‑only      | Footprint bajo + SQL    | Kafka/EventStoreDB (escala) |
| Grafo       | CTE recursivo + `task_edges`    | Sencillo + RAM baja     | Neo4j/FalkorDB al escalar   |
| VDB         | pgvector                        | Co‑ubicado y suficiente | Qdrant/Milvus si GPU/escala |
| Colas       | Redis (locks/rate/colas cortas) | Ligero                  | RabbitMQ/Kafka              |
| API         | REST + gRPC                     | Facilidad + binario     | Solo REST                   |
| Policies    | RLS + JWT + policy simple       | Seguro y simple         | OPA/Cedar completo          |

---

## 4) Datos y Contratos

### 4.1 DDL esencial (Postgres)

```sql
CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  tenant_id TEXT NOT NULL,
  project_id TEXT NOT NULL,
  plan_id TEXT,
  task_id TEXT,
  event_type TEXT NOT NULL,
  payload JSONB NOT NULL,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX ON events (tenant_id, project_id, created_at);
CREATE INDEX ON events (plan_id, created_at);
CREATE INDEX ON events (task_id, created_at);

CREATE TABLE tasks_read (
  tenant_id TEXT, project_id TEXT, task_id TEXT,
  status TEXT, owner_agent_id TEXT, updated_at TIMESTAMPTZ,
  PRIMARY KEY (tenant_id, project_id, task_id)
);

CREATE TABLE task_edges (
  tenant_id TEXT, project_id TEXT,
  from_task TEXT, to_task TEXT,
  PRIMARY KEY (tenant_id, project_id, from_task, to_task)
);

CREATE TABLE artifacts_read (
  tenant_id TEXT, project_id TEXT, artifact_id TEXT,
  path TEXT, hash_curr TEXT, status TEXT,
  PRIMARY KEY (tenant_id, project_id, artifact_id)
);

CREATE TABLE resource_locks (
  tenant_id TEXT, project_id TEXT, path TEXT,
  locked_by TEXT, locked_at TIMESTAMPTZ DEFAULT now(),
  PRIMARY KEY (tenant_id, project_id, path)
);
```

**CTE recursivo (dependencias descendientes por fallo):**

```sql
WITH RECURSIVE affected(task_id) AS (
  SELECT to_task FROM task_edges
   WHERE tenant_id=$1 AND project_id=$2 AND from_task=$3
  UNION
  SELECT te.to_task FROM task_edges te
  JOIN affected a ON te.tenant_id=$1 AND te.project_id=$2 AND te.from_task=a.task_id
)
SELECT * FROM tasks_read
 WHERE tenant_id=$1 AND project_id=$2 AND task_id IN (SELECT task_id FROM affected);
```

### 4.2 Artefactos `.md` (front‑matter)

```markdown
---
artifact_id: artf-...
tenant_id: t-...
project_id: p-...
task_id: T-123.4.1
version: 7
hash_prev: sha256:...
hash_curr: sha256:...
signer: agent-42
schema_version: 1.0
status: in_progress
created_at: 2025-11-08T08:42:00-05:00
updated_at: 2025-11-08T08:58:12-05:00
links:
  - type: depends_on
    ref: T-123.4.0
---
# Resumen
...
## Worklog
- 08:45: Añadido validador.
## Riesgos
...
## Próximo paso
...
```

### 4.3 Catálogo de eventos ES

`IntentDetected, PlanCreated, TaskPlanned, TaskReady, TaskStarted, ArtifactProduced, ArtifactUpdated, ErrorDetected, LockGranted, LockDenied, ResourceReleased, AuditFailed, ReplanRequired, TaskCompleted, PlanCompleted`.

### 4.4 Protocolo de mensajes (Envelope + Content)

**Envelope**

```json
{
  "message_id":"uuid","conversation_id":"uuid","sender_id":"notification_team_01",
  "receiver_id":"team_leader_A_02","timestamp":"2025-11-08T09:01:00-05:00",
  "performative":"HALT","schema_version":"1.0",
  "tenant_id":"t-medluxe","project_id":"p-q4-2025",
  "nl_summary":"Detener cadena por fallo en T-123"
}
```

**Content**

```json
{
  "task_id":"T-123","error_code":"E_DEPENDENCY_FAILED",
  "message":"Fallo en dependencia T-120","context_artifact_id":"artf-...",
  "replan_scope":"sub_graph"
}
```

### 4.5 Perfiles por app (`profile.yaml`)

```yaml
profile_id: medluxe_v1
models:
  reasoning: gpt-4o-mini
  extraction: gpt-4o-mini
  creative: gpt-4o
rag:
  collections:
    - name: ctx_medluxe_q4
      filters: {tenant_id: t-medluxe, project_id: p-q4}
policies:
  tools_allowed: [mcp.notion, mcp.drive, mcp.http]
  rate_limits:
    mcp.http: {rpm: 60}
    mcp.notion: {rpm: 30}
  quotas:
    tokens_month: 5_000_000
```

---

## 5) APIs públicas (REST/gRPC)

**REST:**

* `POST /v1/intents:detect` → `{text, tenant_id, project_id}` → `{intent, slots}`
* `POST /v1/plans:create` → `{objective, inputs, tenant_id, project_id}` → `{plan_id, dag_summary}`
* `POST /v1/plans/{plan_id}:run` → async (SSE/Webhook)
* `GET /v1/plans/{plan_id}/status`
* `GET /v1/artifacts/{artifact_id}`
* `POST /v1/tools/{tool}:invoke`

**gRPC (extracto):**

```proto
service Orchestrator {
  rpc DetectIntent(DetectIntentRequest) returns (DetectIntentResponse);
  rpc CreatePlan(CreatePlanRequest) returns (CreatePlanResponse);
  rpc RunPlan(RunPlanRequest) returns (RunPlanResponse);
}
message DetectIntentRequest { string tenant_id=1; string project_id=2; string text=3; }
message DetectIntentResponse { string intent=1; map<string,string> slots=2; }
```

---

## 6) MCP multi‑tenant

* `InitializeSession` exige `{tenant_id, project_id, role, profile_id}`.
* Policy engine verifica `tools_allowed`, **quotas**, **rate‑limits** y **scopes**.
* **Locks**: `LockResourceRequest` → CTX valida/otorga (`LockGranted/Denied`).
* **Sandbox**: contenedores efímeros (gVisor/Firecracker opcional), egress controlado, límites CPU/RAM.

---

## 7) UI/UX — Consola de Plataforma

### 7.1 Principios

* Claridad jerárquica (Mando→Plan→Tareas→Artefactos→Eventos).
* Estado en vivo (SSE) con temporalidad y filtros por tenant/proyecto.
* Acciones seguras (confirmaciones, scopes y *role‑based UI*).
* Accesibilidad (WCAG AA), i18n (ES/EN), dark/light.

### 7.2 Design System

* Tipografía: Inter/Source Sans; escalas 12/14/16/20/24/32.
* Colores: neutrales + severidad (info/warn/error/critical).
* Componentes: AppShell, Breadcrumb, DAGView (layers), ArtifactViewer (front‑matter + diff), EventStream, LockPanel, Metrics.

### 7.3 Flujos clave

* Vista **Plan** (DAG + KPIs + riesgos).
* Vista **Artefacto** (hash chain, validación, links).
* Vista **Notificaciones** (cola, silenciamiento, SLA, destinos dirigidos).

---

## 8) Seguridad

* JWT con claims `{tenant_id, roles, scopes}`; renovación corta.
* **RLS** en Postgres y `SET app.tenant_id` en cada conexión.
* TLS externo obligatorio; mTLS interno opcional.
* Gestión de secretos por tenant (vault/env segregado).

---

## 9) Observabilidad

* Campos obligatorios: `{tenant_id, project_id, plan_id, task_id}`.
* Métricas: latencia por evento, throughput, error por tool, backlog de colas.
* Alertas: SLA violado, `AuditFailed`, `ReplanRequired`, locks > N min.

---

## 10) Despliegue (VPS 4 vCPU / 8 GB)

### 10.1 Composición mínima

* Postgres 15 + pgvector (2–3 GB)
* Redis 7 (locks/rate/colas) (200–300 MB)
* HAAS Core (Gateway/ORQ/PLAN/NOTI) (1–1.5 GB)
* MCP (ligero)
* File Store (artefactos `.md`)

### 10.2 `docker-compose` (esqueleto)

```yaml
services:
  gateway:
    image: yourrepo/haas-gateway:latest
    env_file: .env
    depends_on: [postgres, redis]
    ports: ["443:8443"]
  planner:
    image: yourrepo/haas-planner:latest
    env_file: .env
    depends_on: [postgres, redis]
  mcp:
    image: yourrepo/haas-mcp:latest
    env_file: .env
  postgres:
    image: postgres:15
    environment: [POSTGRES_PASSWORD=***]
    volumes: ["pgdata:/var/lib/postgresql/data"]
  redis:
    image: redis:7
volumes: { pgdata: {} }
```

### 10.3 Checklists

* [ ] Backups nocturnos de `pg_dump` + artefactos.
* [ ] Rotación de logs.
* [ ] Liveness/readiness probes.
* [ ] Pruebas anti‑fuga cross‑tenant.

---

## 11) Pruebas y QA

* **Unitarias**: esquemas `.md`, parsers, rate‑limiters, validadores de eventos.
* **Integración**: ES→Proyección→Consulta; `LockResource` atómico; RLS.
* **E2E**: intención→brief→plan→artefactos→notificaciones.
* **Chaos/DR**: fallas de tool/red; auto‑reparación.

---

## 12) SDKs cliente

**JS/TS**

```ts
const client = new HaasClient({ baseUrl, token, tenantId, projectId });
const { intent } = await client.detectIntent(text);
```

**C# (.NET)**

```csharp
var client = new HaasClient(baseUrl, token, tenantId, projectId);
var resp = await client.DetectIntentAsync(new DetectIntentRequest{ Text = text });
```

**Python**

```python
client = HaasClient(base_url, token, tenant_id, project_id)
intent = client.detect_intent(text)
```

---

## 13) Integración con apps existentes

1. Crear **tenant** y **project** por app/campaña.
2. Configurar `profile.yaml` (modelos, tools, RAG).
3. Integrar SDK/REST/gRPC (intents, plan, run, artifacts, tools).
4. Webhooks/SSE para progreso.
5. Envío de contenidos a tools MCP (Drive/Notion/HTTP).

---

## 14) Historias de usuario (selección)

* **Como** líder de producto **quiero** crear un plan desde una intención **para** ver un DAG y su ejecución con artefactos auditables.
* **Como** SRE **quiero** bloquear recursos con mutex **para** evitar condiciones de carrera.
* **Como** auditor **quiero** validar `.md` con hash encadenado **para** garantizar integridad.
* **Como** app externa **quiero** consumir la API con `{tenant_id, project_id}` **para** mantener aislamiento de datos.

**Criterios de aceptación**: eventos correctos en ES; proyecciones actualizadas; RLS efectiva; notificaciones dirigidas; métricas visibles.

---

## 15) Plan por fases + cronograma

**F0 — Fundaciones (Semana 1)**: DDL Postgres, RLS, validadores `.md`, SDK “hello world”.

**F1 — Mando & Control (Semanas 2–3)**: Orquestador (intents), Ingeniería de Prompts (APE+CoT), Planificador (DAG+contratos), NOTI (SSE/Webhooks).

**F2 — MCP & Tools (Semanas 3–4)**: MCP multi‑tenant + policy; 2–3 tools; Redis (rate/locks/colas).

**F3 — Memoria avanzada (Semanas 4–5)**: CQRS robusto; CTE recursivo; pgvector; consola UI v1.

**F4 — Resiliencia & QA (Semana 6)**: circuit breakers; chaos drills; E2E; canarios cross‑tenant.

**F5 — Escalado (Opcional)**: Neo4j/Kafka; OPA/Cedar; GPU RAG.

---

## 16) Hardening y Manual de operación

* **Hardening**: TLS fuerte, mTLS opcional, políticas de secretos, rotación de llaves, límites de CPU/RAM por servicio, auditoría de permisos tools MCP.
* **Runbooks**: SLA violado, `AuditFailed`, lock retenido, reproyección CQRS, restore de backups.
* **DR**: snapshots diarios + verificación de restore mensual.

---

## 17) Plantillas adicionales

**`brief.md`**

```markdown
---
artifact_type: brief
owner_team: Prompts
acceptance_criteria:
  - Objetivos claros y medibles
  - Artefactos requeridos listados
---
# Objetivo
...
# Alcance
...
# Entradas requeridas
...
# Artefactos a generar
- content_calendar.md
- paid_plan.md
```

**`team_report.md`**

```markdown
---
artifact_type: team_report
team: PaidMedia
period: 2025‑11‑01..2025‑11‑15
kpis:
  spend: 1200
  cpl: 3.1
  leads: 180
---
# Resumen
...
# Riesgos/Acciones
...
```

---

## 18) Hoja de ruta de escalado

* **DB**: particionamiento por tenant/proyecto; migración de grafo a Neo4j; cachés de lectura; colas Kafka.
* **Ejecución**: pools dedicados por dominio; autoscaling vertical/horizontal; GPU opcional para RAG/visión.
* **Governanza**: OPA/Cedar; *feature flags* por profile; HRL para promoción dinámica de líderes.

---

> **Estado:** Documento ejecutable listo para llevar a repos/contratos gRPC y `docker-compose`. Puede integrarse directamente a tu stack .NET 8/WPF/gRPC/SQLite/Firestore/MCP o alternativa FastAPI.
